{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6226ee5-8bba-4333-ab08-815e3a3fc794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65130833-87bf-43c2-a13f-6c1e53c3d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5.1 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b916f3ef-1b8d-4ef8-95e3-f66cb96efa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]    #1 is abusive, 0 not\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33f118d0-e38a-40df-8dd7-037b1ef8b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "##创建词表集合\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set()\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae34329-8d40-4203-8d15-28b1c0cae0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##判断某一列单词是否出现在词表中\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58263071-7488-462d-9c80-9dbe952f27a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "listPosts,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d843dd-86eb-444d-b8cd-37888e11e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21487b41-8e64-44b0-a772-86dfca11745f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223e989d-3ca7-401d-b098-14d117bb90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5.2训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a361690-c0f4-4051-a4f6-8ad793fc25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##朴素贝叶斯训练函数\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num,p1Num = ones(numWords),ones(numWords)\n",
    "    p0Denom,p1Denom = 2.0,2.0\n",
    "    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])     \n",
    "    p1Vect = log(p1Num/p1Denom)\n",
    "    p0Vect = log(p0Num/p0Denom)    \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80554c66-f269-4025-bf82-a0d6d642cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "350233e0-6a77-450a-bc9d-49381f7e9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAb = trainNB0(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62e94eb4-4a1a-4a7b-b07a-c78d2f613c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99291550-9e61-417d-b529-dcbf42f44337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.56494936, -3.25809654, -1.87180218, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -2.15948425, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -3.25809654, -3.25809654, -3.25809654,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -3.25809654])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64f8b865-8a59-4783-9606-f17cf4e99e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.04452244, -2.35137526, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -1.65822808, -3.04452244, -1.94591015, -2.35137526, -2.35137526,\n",
       "       -3.04452244, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -1.94591015, -3.04452244, -2.35137526, -2.35137526, -2.35137526,\n",
       "       -2.35137526, -3.04452244, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -2.35137526])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391998e-9a83-45d4-89db-c6b8f5004d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf45ef97-df51-4e27-9b53-bc06a490a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #概率相乘，再取对数后，转为直接相加\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4dd7b1d-9466-4eb8-9665-bc2410c91508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    \n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    \n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(trainMat,listClasses)\n",
    "    \n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = setOfWords2Vec(myVocabList,testEntry)\n",
    "    \n",
    "    print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f65ed6e-4cfe-4bfb-ab4f-4cb6b21f6c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as: 0\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a7b04-9b23-4ac7-9dda-c7bf68f05744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "712be622-cd1f-43cf-899f-7dd804b368d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.5.4 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8179caf5-4d33-40cc-8b36-d3ffd83e3dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofWords2VecMN(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95979742-06a6-484e-a971-79b76510c031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf90b600-23d8-4c50-b9f5-cf593796ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_path=Path('D:\\\\python_algorithm\\\\machinelearinginaction\\\\《机器学习实战》Python3代码\\\\Ch04')\n",
    "\n",
    "fr = open(data_path / 'email/ham/6.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bf56204-3985-4606-880c-9ac7a191b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regEx = re.compile('\\\\W*')\n",
    "listOfEokens = regEx.split(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9bb117-1e98-440b-a103-5bfcc56791a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5e787d3-8156-4037-90fd-8f2ae7fc89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.6.2 测试算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b3f107d-5713-4a9e-a2a8-f0b758a623d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "704e0528-39f3-436f-832a-d75b753f02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1,26):\n",
    "        file_name = 'email/spam/%d.txt'%i\n",
    "        wordList = textParse(open(data_path / file_name,encoding=\"ISO-8859-1\").read())     #加入指定的编码格式\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        \n",
    "        file_name = 'email/ham/%d.txt'%i\n",
    "        wordList = textParse(open(data_path / file_name,encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    \n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = range(50)\n",
    "    testSet = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(list(trainingSet)[randIndex])                    #转为list\n",
    "        \n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    \n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "        \n",
    "    p0V,p1V,pSpam = trainNB0(trainMat,trainClasses)\n",
    "    \n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classifyNB(wordVector,p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is:',float(errorCount)/len(testSet))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae30d576-c5f7-43ed-8ba6-facae3171829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.6\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7da49-1b42-4f59-8ee3-cf6077acb343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81216b40-70f1-49c4-83f4-b1a0cc8ee560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17926e-88f4-4a60-99a8-8cb0960088f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
